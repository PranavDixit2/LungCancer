{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBYwevlaZhEC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaiUkWiE6D0e",
        "outputId": "2915b74f-f517-4354-a2e5-3b00b28aab6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/quynhlecl/lung-cancer-x-ray?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.28G/2.28G [00:09<00:00, 249MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/quynhlecl/lung-cancer-x-ray/versions/1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"quynhlecl/lung-cancer-x-ray\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the root dataset path\n",
        "root_dataset_path = \"/root/.cache/kagglehub/datasets/quynhlecl/lung-cancer-x-ray/versions/1/chest_xray_lung/\"\n",
        "\n",
        "train_dataset_path = os.path.join(root_dataset_path, \"train\")\n",
        "test_dataset_path = os.path.join(root_dataset_path, \"test\")\n",
        "val_dataset_path = os.path.join(root_dataset_path, \"val\")\n"
      ],
      "metadata": {
        "id": "uV9zy0706UHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ka4MdIJz6dyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to explore the dataset\n",
        "def explore_dataset(dataset_path):\n",
        "    class_counts = {}\n",
        "    total_images = 0\n",
        "\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for folder in dirs:\n",
        "            folder_path = os.path.join(root, folder)\n",
        "            num_files = len(os.listdir(folder_path))\n",
        "            class_counts[folder] = num_files\n",
        "            total_images += num_files\n",
        "\n",
        "    print(\"Dataset Overview:\")\n",
        "    print(f\"Total Classes: {len(class_counts)}\")\n",
        "    print(f\"Total Images: {total_images}\")\n",
        "    print(\"Class Distribution:\")\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"  {cls}: {count}\")\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "class_counts = explore_dataset(root_dataset_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hko_iaCz6eb4",
        "outputId": "7855790c-2cfc-4174-f6c9-c1d87a14628a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Overview:\n",
            "Total Classes: 6\n",
            "Total Images: 11727\n",
            "Class Distribution:\n",
            "  test: 2\n",
            "  val: 2\n",
            "  train: 2\n",
            "  chest_xray: 3\n",
            "  NORMAL: 1341\n",
            "  Cancer: 3875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights for imbalanced data\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(list(class_counts.keys())),\n",
        "    y=np.concatenate([[k] * v for k, v in class_counts.items()])\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n"
      ],
      "metadata": {
        "id": "bV4tXWOG6knh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "train_gen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Data Generators\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    train_dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    shuffle=True\n",
        ")\n",
        "val_generator = val_test_gen.flow_from_directory(\n",
        "    val_dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "test_generator = val_test_gen.flow_from_directory(\n",
        "    test_dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    shuffle=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo_QiA4j6qLD",
        "outputId": "ce345b55-273f-4033-c643-a616a3737c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition using ResNet50\n",
        "def create_model():\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHTCmsK6u_T",
        "outputId": "72a1c9b0-68f5-4e5d-d97b-11a9db503991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 2048)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               262272    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23850242 (90.98 MB)\n",
            "Trainable params: 262530 (1.00 MB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Callbacks for training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model_resnet.h5', save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
        "\n",
        "\n",
        "def extract_data_and_labels(generator):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for _ in range(len(generator)):\n",
        "        images, batch_labels = generator.next()\n",
        "        data.append(images)\n",
        "        labels.append(batch_labels)\n",
        "    return np.vstack(data), np.concatenate(labels)\n",
        "\n",
        "# Extract data and labels\n",
        "train_data, train_labels = extract_data_and_labels(train_generator)\n",
        "\n",
        "# Initialize KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform K-Fold Cross-Validation\n",
        "fold_no = 1\n",
        "for train_idx, val_idx in kfold.split(train_data, train_labels):\n",
        "    print(f\"Training Fold {fold_no}...\")\n",
        "\n",
        "    # Create data generators for the current fold\n",
        "    train_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow(\n",
        "        train_data[train_idx], train_labels[train_idx], batch_size=32\n",
        "    )\n",
        "    val_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow(\n",
        "        train_data[val_idx], train_labels[val_idx], batch_size=32\n",
        "    )\n",
        "\n",
        "    # Create a new model instance\n",
        "    model = create_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_data_gen,\n",
        "        epochs=10,\n",
        "        validation_data=val_data_gen,\n",
        "        callbacks=[early_stopping, model_checkpoint]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_acc = model.evaluate(val_data_gen)\n",
        "    print(f\"Fold {fold_no} - Validation Accuracy: {val_acc:.2f}\")\n",
        "\n",
        "    fold_no += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nwaHytt6zaa",
        "outputId": "3403e371-b13a-4a1e-95e7-29a1e7146ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Fold 1...\n",
            "Epoch 1/10\n",
            "131/131 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.9010"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r131/131 [==============================] - 898s 7s/step - loss: 0.2530 - accuracy: 0.9010 - val_loss: 0.2419 - val_accuracy: 0.8956\n",
            "Epoch 2/10\n",
            "131/131 [==============================] - 897s 7s/step - loss: 0.1535 - accuracy: 0.9382 - val_loss: 0.1325 - val_accuracy: 0.9492\n",
            "Epoch 3/10\n",
            "131/131 [==============================] - 878s 7s/step - loss: 0.1240 - accuracy: 0.9506 - val_loss: 0.1210 - val_accuracy: 0.9569\n",
            "Epoch 4/10\n",
            "131/131 [==============================] - 882s 7s/step - loss: 0.0984 - accuracy: 0.9619 - val_loss: 0.1298 - val_accuracy: 0.9473\n",
            "Epoch 5/10\n",
            "131/131 [==============================] - 876s 7s/step - loss: 0.0999 - accuracy: 0.9609 - val_loss: 0.1057 - val_accuracy: 0.9636\n",
            "Epoch 6/10\n",
            "131/131 [==============================] - 876s 7s/step - loss: 0.0963 - accuracy: 0.9640 - val_loss: 0.1200 - val_accuracy: 0.9550\n",
            "Epoch 7/10\n",
            "131/131 [==============================] - 876s 7s/step - loss: 0.0766 - accuracy: 0.9703 - val_loss: 0.1350 - val_accuracy: 0.9502\n",
            "Epoch 8/10\n",
            "131/131 [==============================] - 875s 7s/step - loss: 0.0785 - accuracy: 0.9720 - val_loss: 0.1544 - val_accuracy: 0.9425\n",
            "Epoch 9/10\n",
            "131/131 [==============================] - 883s 7s/step - loss: 0.0733 - accuracy: 0.9744 - val_loss: 0.1079 - val_accuracy: 0.9588\n",
            "Epoch 10/10\n",
            "131/131 [==============================] - 878s 7s/step - loss: 0.0655 - accuracy: 0.9732 - val_loss: 0.1099 - val_accuracy: 0.9655\n",
            "33/33 [==============================] - 175s 5s/step - loss: 0.1057 - accuracy: 0.9636\n",
            "Fold 1 - Validation Accuracy: 0.96\n",
            "Training Fold 2...\n",
            "Epoch 1/10\n",
            "131/131 [==============================] - 885s 7s/step - loss: 0.3305 - accuracy: 0.8871 - val_loss: 0.1419 - val_accuracy: 0.9501\n",
            "Epoch 2/10\n",
            "131/131 [==============================] - 894s 7s/step - loss: 0.1494 - accuracy: 0.9425 - val_loss: 0.1227 - val_accuracy: 0.9530\n",
            "Epoch 3/10\n",
            "131/131 [==============================] - 884s 7s/step - loss: 0.1436 - accuracy: 0.9399 - val_loss: 0.1487 - val_accuracy: 0.9386\n",
            "Epoch 4/10\n",
            "131/131 [==============================] - 889s 7s/step - loss: 0.1246 - accuracy: 0.9542 - val_loss: 0.1495 - val_accuracy: 0.9358\n",
            "Epoch 5/10\n",
            " 40/131 [========>.....................] - ETA: 8:12 - loss: 0.1035 - accuracy: 0.9648"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# Classification Report and Metrics\n",
        "y_pred = np.argmax(model.predict(test_generator), axis=1)\n",
        "y_true = test_generator.classes\n",
        "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
        "\n",
        "# AUC-ROC Score\n",
        "auc = roc_auc_score(y_true, y_pred)\n",
        "print(f\"AUC-ROC: {auc:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(),\n",
        "            yticklabels=test_generator.class_indices.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "z65OA5P6620e",
        "outputId": "6b628d11-8438-4bf2-bec4-bec2db380966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-42acda84f45c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_acc:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Classification Report and Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of Training Histories\n",
        "for idx, history in enumerate(histories):\n",
        "    plt.plot(history.history['accuracy'], label=f'Fold {idx+1} Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label=f'Fold {idx+1} Validation Accuracy')\n",
        "\n",
        "plt.title('Cross-Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oCxQzVpO66S_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}